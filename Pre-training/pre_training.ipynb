{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7430c6c9-2c70-45b2-b2c1-73971782e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.23.4\n",
      "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/workspace/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed numpy-1.23.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn\n",
    "!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e19eb5e-dc27-4131-aa2f-59f1361ff795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM, BatchEncoding,Trainer, TrainingArguments\n",
    "import torch\n",
    "from math import ceil\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import wandb\n",
    "from sys import getsizeof\n",
    "from torch.cuda import device_count\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "557c46a6-bd4b-4c77-9c62-c61f9f9c2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(device_count()):\n",
    "    cuda.select_device(i).reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eaa9d97-b470-422f-b4b6-00e9989ebd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_array(arr, chunk_size):\n",
    "    \"\"\"Chunks an array into chunks of specified size.\"\"\"\n",
    "    # For every index in arr stepping by chunk_size, slice arr from that index to index + chunk_size\n",
    "    return [arr[i:i + chunk_size] for i in range(0, len(arr), chunk_size)]\n",
    "\n",
    "def tokenize(element,context_length):\n",
    "    outputs = tokenizer(\n",
    "        element['text'].to_list(),\n",
    "        truncation=False\n",
    "    )\n",
    "    #print(outputs[2].attention_mask)\n",
    "    input_batch = []\n",
    "    for i in range(len(element)):\n",
    "        temp = outputs[i].ids.copy()\n",
    "        temp.append(tokenizer.encode(tokenizer.eos_token)[1])\n",
    "        #outputs[i].ids.append(tokenizer.encode(tokenizer.eos_token)[1])\n",
    "        input_batch += temp\n",
    "    print(len(input_batch))\n",
    "    chunks = chunk_array(input_batch, context_length)\n",
    "    if len(chunks[-1]) != context_length:\n",
    "        del chunks[-1]\n",
    "    #last_chunk = chunks[-1]\n",
    "    #n_pads = context_length - len(last_chunk)\n",
    "    #last_chunk += [tokenizer.encode(tokenizer.pad_token)[1]]*n_pads\n",
    "    \n",
    "    wrapped_chunks = [BatchEncoding({\"input_ids\":torch.tensor(chunk), \"attention_mask\":torch.tensor([1]*context_length)}) for chunk in chunks]\n",
    "    \n",
    "    return wrapped_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02156d6a-7e41-4158-8e54-dee38b04abd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb3e4c8c9184c92bc70bb3de6321576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-2-13b-hf'#'artifacts/checkpoint-cme0pvg4:v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          token='')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             token = '',\n",
    "                                             torch_dtype=torch.float32,\n",
    "                                             device_map='auto')\n",
    "\n",
    "context_length = 4096\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d8b585-a838-448d-8dbd-b9b8d7843759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248603\n",
      "{'input_ids': tensor([    1,  4721,   862,  ...,   262,  6352, 20642]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(\"pre_train_data.json\",compression=None)\n",
    "train, test = train_test_split(data,test_size = 0.05,random_state = 42)\n",
    "chunks = tokenize(test,context_length)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8c6f50-a65a-438e-8bc6-a35df31bee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8799651\n",
      "input_ids shape: torch.Size([8, 4096])\n",
      "attention_mask shape: torch.Size([8, 4096])\n",
      "labels shape: torch.Size([8, 4096])\n"
     ]
    }
   ],
   "source": [
    "train_chunks = tokenize(train,context_length)\n",
    "#train_chunks = train_chunks[4:]\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator([chunks[i] for i in range(8)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9a218-9dde-4468-9748-55e6c6ab97d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2420549392700195\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(inputs):\n",
    "    #inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    #print(type(inputs))\n",
    "    inputs = inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    log_likelihood = outputs.loss * inputs[\"input_ids\"].shape[1]\n",
    "    perplexity = torch.exp(log_likelihood / inputs[\"input_ids\"].shape[1])\n",
    "    return perplexity.item()\n",
    "perp = list()\n",
    "for i,chunk in enumerate(train_chunks):\n",
    "    perp.append(calculate_perplexity(chunk))\n",
    "    if i % 100 == 0:\n",
    "        print(i,perp[i])\n",
    "perplexity = pd.DataFrame(perp)\n",
    "perplexity.to_json('../../data/perplexity.json',force_ascii=False,compression=None)\n",
    "print(np.mean(perp))\n",
    "print(np.var(perp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ccc8852-f844-4b8a-b2c8-c7550367f7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248    14.434236\n",
      "868     13.033507\n",
      "1246    12.901219\n",
      "1989    12.872516\n",
      "314     11.619267\n",
      "1694    11.600303\n",
      "1247    11.349825\n",
      "1599    11.312692\n",
      "1358    11.309628\n",
      "1123    11.222836\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(perplexity[0].nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bda37d3-478b-48ed-b76d-e8e68f898cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"LLM-pre-training\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"pre_training_April_9\",\n",
    "    per_device_train_batch_size=1,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_00,\n",
    "    logging_steps=1,\n",
    "    #gradient_accumulation_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=3e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    "    adam_beta2 = 0.95,\n",
    "    adam_epsilon = 1e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_chunks,\n",
    "    eval_dataset=chunks\n",
    ")\n",
    "trainer.is_model_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12c40e-0529-492f-86f8-287d9bce9d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 05:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='469' max='2148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 469/2148 2:22:55 < 8:33:49, 0.05 it/s, Epoch 0.22/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('../../data/pretraining/domain_adaptation')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c3440c-f2b9-4bc6-a760-4c842ea06995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments,GenerationConfig, AutoTokenizer, LlamaConfig\n",
    "from peft import LoraModel, LoraConfig, get_peft_model, PeftModel\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "import wandb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "936c6c11-d419-4be4-99c6-5d0ff5d23ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = '''[INST] <<SYS>>\n",
    "Du är en hjälpsam medicinsk assistent som hjälper läkare och sjuksköterskor genom att svara på frågor.\n",
    "Svara på svenska.\n",
    "<</SYS>>\n",
    "Nedan ges en fråga eller ett medicinskt begrepp.\n",
    "<fråga>'''\n",
    "\n",
    "prompt2 = '''\n",
    "</fråga>\n",
    "Svara på frågan eller förklara begreppet.\n",
    "[/INST]'''\n",
    "\n",
    "data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "formatted = []\n",
    "for index, row in data.iterrows():\n",
    "    #print(row)\n",
    "    elem = {'text':f\"<s>{prompt1}\\n{row['Question']}{prompt2}\\n{row['Answer']} </s>\"}\n",
    "    formatted.append(elem)\n",
    "    \n",
    "\n",
    "train, test = train_test_split(formatted,test_size = 0.05,random_state = 42)\n",
    "train = Dataset.from_list(train)\n",
    "test = Dataset.from_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3191521-fa8a-456e-bb65-904f468d720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /workspace/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/DATX05/Pre-training/wandb/run-20240423_122045-0mevglyj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/master-t/DATX05-Pre-training_DATX05_Pre-training/runs/0mevglyj' target=\"_blank\">qa_no_domain</a></strong> to <a href='https://wandb.ai/master-t/DATX05-Pre-training_DATX05_Pre-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/master-t/DATX05-Pre-training_DATX05_Pre-training' target=\"_blank\">https://wandb.ai/master-t/DATX05-Pre-training_DATX05_Pre-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/master-t/DATX05-Pre-training_DATX05_Pre-training/runs/0mevglyj' target=\"_blank\">https://wandb.ai/master-t/DATX05-Pre-training_DATX05_Pre-training/runs/0mevglyj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(name=\"qa_no_domain\")\n",
    "os.environ[\"WANDB_PROJECT\"] = \"LLM-QA\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2f739e-2d77-4b31-b7a1-3f667f3f6a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0b13b1fcac411abb29e612518ad36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c58832c19cb45d5b0c780d374bb86e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"qa_April_23\",\n",
    "    auto_find_batch_size=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=700,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    bf16=False,\n",
    "    warmup_steps=2000,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=80000,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    "    adam_beta2 = 0.95,\n",
    "    adam_epsilon = 1e-5\n",
    "    #neftune_noise_alpha=5\n",
    ")\n",
    "response_template = \"\\n[/INST]\"\n",
    "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer,mlm=False)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    packing=False,\n",
    "    max_seq_length=4096,\n",
    "    dataset_text_field='text',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17f40e-0763-402e-864a-12f75f72ba2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 37:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1772' max='8340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1772/8340 1:10:42 < 4:22:22, 0.42 it/s, Epoch 0.21/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.760900</td>\n",
       "      <td>1.790056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.531200</td>\n",
       "      <td>2.108809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('../../data/pretraining/qa_nodomain')\n",
    "#trainer.model.save_pretrained('../../../data/finetuned/lora_no_domain_adaptation/model')\n",
    "#trainer.tokenizer.save_pretrained('../../../data/finetuned/lora_no_domain_adaptation/tokenizer')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8062c162-7ca5-4c9d-bc3d-44622964f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        bias='none',\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        use_rslora=True\n",
    "    )\n",
    "model = get_peft_model(model, config, \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b122c2c9-6499-44a9-b601-0938fc52c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 52,428,800 || all params: 13,068,303,360 || trainable%: 0.4011905643426998\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e139fa4-be01-4d7b-acc3-06f19ed9a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = '''[INST] <<SYS>>\n",
    "Du är en hjälpsam medicinsk assistent som hjälper läkare och sjuksköterskor genom att sammanfatta information om patienter.\n",
    "Svara på svenska.\n",
    "<</SYS>>\n",
    "Nedan ges anamnes för en patient under en dag\n",
    "<anamnes>'''\n",
    "\n",
    "prompt2 = '''\n",
    "</anamnes>\n",
    "Du ska plocka ut information som passar i mallen nedan. Undvik onödig information och plocka endast ut sådant som rör varje rubrik. Om relevant information saknas så lämnar du rubriken tom.\n",
    "Formattera ditt svar enligt mallen. Ingen information kan finnas under flera rubriker.\n",
    "<mall>\n",
    "*Sjukdomshistoria (Patientens diagnoser, sjukdomshistorik och riskfaktorer (t.ex. sjukdomar i familjen))*\n",
    "\n",
    "*Sökorsaker (Patientens symtom och/eller datum för ingrepp)*\n",
    "\n",
    "*Åtgärder (Planerade undersökningar, behandlingar och åtgärder)*\n",
    "</mall>\n",
    "[/INST]'''\n",
    "data = pd.read_parquet(\"synthetic_229_corrected.parquet\")\n",
    "\n",
    "formatted = []\n",
    "for index, row in data.iterrows():\n",
    "    #print(row)\n",
    "    elem = {'text':f\"<s> {prompt1}\\n{row['description']}{prompt2}\\n{row['summary']} </s>\"}\n",
    "    formatted.append(elem)\n",
    "    \n",
    "\n",
    "train, test = train_test_split(formatted,test_size = 0.05,random_state = 42)\n",
    "train = Dataset.from_list(train)\n",
    "test = Dataset.from_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbcfabf-e14a-47dc-90d5-9c3924e304f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(name='lora_nodomain')\n",
    "os.environ[\"WANDB_PROJECT\"] = \"LLM-LoRA\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1f451-61ad-4aa7-b66f-21798d68c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"lora_April_23\",\n",
    "    auto_find_batch_size=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=40,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    bf16=False,\n",
    "    warmup_steps=30,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-4,\n",
    "    save_steps=8000,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    "    adam_beta2 = 0.95,\n",
    "    adam_epsilon = 1e-5,\n",
    "    neftune_noise_alpha=5\n",
    "    #remove_unused_columns=False\n",
    ")\n",
    "response_template = \"\\n[/INST]\"\n",
    "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer,mlm=False)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    packing=False,\n",
    "    max_seq_length=4096,\n",
    "    dataset_text_field='text',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d77c00-6211-4df4-8a57-5da24bef462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('../../data/finetuned/lora_mega_no_domain')\n",
    "#trainer.model.save_pretrained('../../../data/finetuned/lora_no_domain_adaptation/model')\n",
    "#trainer.tokenizer.save_pretrained('../../../data/finetuned/lora_no_domain_adaptation/tokenizer')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ab4eb24-b3a0-4eb2-aa97-bbbee5ec451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217.0\n"
     ]
    }
   ],
   "source": [
    "print(len(train)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a08ef-52b5-494a-b446-7ebe28cd5749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
